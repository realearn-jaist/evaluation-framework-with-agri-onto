{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjVHmgWrEgWE",
        "outputId": "a2ddff34-d1c6-45a5-a5c0-5d2346834971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "with open('/content/crop ontology to use.owl', 'r') as file:\n",
        "    ontology_data = file.read().splitlines()\n",
        "\n",
        "# Preprocess the data and split into sentences\n",
        "sentences = [sentence.split() for sentence in ontology_data]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "jLfGnpksEnLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('ontology_embedding_model.bin')"
      ],
      "metadata": {
        "id": "HkgiRJK5E3FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get all concept names\n",
        "concept_names = list(model.wv.index_to_key)\n",
        "\n",
        "# Get embeddings for all concepts\n",
        "embeddings = [model.wv[concept_name] for concept_name in concept_names]\n",
        "\n",
        "# Create a pandas DataFrame to hold the embeddings and concept names\n",
        "df = pd.DataFrame(embeddings, index=concept_names)\n",
        "\n",
        "\n",
        "csv_file_path = 'concept_embeddings.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(csv_file_path)"
      ],
      "metadata": {
        "id": "tR2boEIyFUZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_name = '/content/concept_embeddings.csv'\n",
        "\n",
        "# Read the CSV file using Pandas\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "\n",
        "excel_file_name = 'word2vec results.xlsx'"
      ],
      "metadata": {
        "id": "AFrioy-mG2XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the DataFrame to Excel\n",
        "df.to_excel(excel_file_name, index=False)"
      ],
      "metadata": {
        "id": "ZdTneLcFHSDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}